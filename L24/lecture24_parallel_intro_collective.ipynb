{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 24 : Parallel Message Passing and Introduction to Collective Communication"
      ],
      "metadata": {
        "id": "axYjNLDcwqYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 : Parallel Message Passing\n",
        "\n",
        "## Let's revisit the MPI sum code from lecture 22."
      ],
      "metadata": {
        "id": "fBYgdZT8xzjJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLABunvSsK91",
        "outputId": "8d1e11ab-f4ff-4bb5-bb16-c67245a2ce3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi_sum_v1.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile mpi_sum_v1.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "\n",
        "    MPI_Init (&argc, &argv);\n",
        "\n",
        "    // MPI_COMM_WORLD is the default communicator that contains all ranks\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // get N from command line\n",
        "    if (argc < 2) {\n",
        "        printf (\"Command usage : %s %s\\n\",argv[0],\"N\");\n",
        "        return 1;\n",
        "    }\n",
        "    long long N = atoll(argv[1]);\n",
        "\n",
        "    // start the timer\n",
        "    double start_time, end_time;\n",
        "    start_time = MPI_Wtime();\n",
        "\n",
        "    // calculate the sum\n",
        "    long long sum = 0;\n",
        "    for (long long i = 1+rank; i <= N;i+=size) {\n",
        "        sum += i;\n",
        "    }\n",
        "\n",
        "    // all nonzero ranks send their partial sums to rank 0\n",
        "    if (rank == 0) {\n",
        "        long long rank_sum;\n",
        "        MPI_Status status;\n",
        "        for (int source=1;source<size;source++) {\n",
        "            MPI_Recv(&rank_sum,1,MPI_UNSIGNED_LONG_LONG,source,0,MPI_COMM_WORLD,&status);\n",
        "            sum += rank_sum;\n",
        "        }\n",
        "    } else {\n",
        "        int dest = 0;\n",
        "        MPI_Send(&sum,1,MPI_LONG_LONG,dest,0,MPI_COMM_WORLD);\n",
        "    }\n",
        "\n",
        "    // all nonzero ranks receive the final sum from rank 0\n",
        "    if (rank == 0) {\n",
        "        for (int dest = 1;dest < size;dest++) {\n",
        "            MPI_Send(&sum,1,MPI_LONG_LONG,dest,0,MPI_COMM_WORLD);\n",
        "        }\n",
        "    } else {\n",
        "        int src = 0;\n",
        "        MPI_Status status;\n",
        "        MPI_Recv(&sum,1,MPI_LONG_LONG,src,0,MPI_COMM_WORLD,&status);\n",
        "    }\n",
        "\n",
        "    // stop the timer\n",
        "    end_time = MPI_Wtime();\n",
        "\n",
        "    // print results\n",
        "    printf (\"rank %d (of %d) sum = %lld, N*(N+1)/2 = %lld, elapsed time = %.4f seconds\\n\",\n",
        "            rank,size,sum,(N/2)*(N+1),end_time-start_time);\n",
        "\n",
        "    MPI_Finalize();\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here are the results of running version 1 on matrix with N equal to 1 million.\n",
        "\n",
        "    $ mpicc -o mpi_sum_v1 mpi_sum_v1.c\n",
        "    $ mpiexec -n 4 ./mpi_sum_v1 1000000\n",
        "    rank 0 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds\n",
        "    rank 1 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds\n",
        "    rank 2 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds\n",
        "    rank 3 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds\n",
        "    $"
      ],
      "metadata": {
        "id": "lPSyR1xtyX4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In version 2, we use parallel communication to reduce the partial sums with result on rank 0.  \n",
        "## For simplicity, we assume that the number of ranks is $2^k$ where $k \\geq 0$."
      ],
      "metadata": {
        "id": "WxZTYjWrzcXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_sum_v2.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "\n",
        "    MPI_Init (&argc, &argv);\n",
        "\n",
        "    // MPI_COMM_WORLD is the default communicator that contains all ranks\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // get N from command line\n",
        "    if (argc < 2) {\n",
        "        printf (\"Command usage : %s %s\\n\",argv[0],\"N\");\n",
        "        return 1;\n",
        "    }\n",
        "    long long N = atoll(argv[1]);\n",
        "\n",
        "    // start the timer\n",
        "    double start_time, end_time;\n",
        "    start_time = MPI_Wtime();\n",
        "\n",
        "    // calculate the sum\n",
        "    long long sum = 0;\n",
        "    for (long long i = 1+rank; i <= N;i+=size) {\n",
        "        sum += i;\n",
        "    }\n",
        "\n",
        "    // use parallel message passing to reduce the partial sums with result on rank 0\n",
        "    // we assume that size = 2^k for some integer k >= 0\n",
        "    int alive = size;\n",
        "    while (alive > 1) {\n",
        "        if (rank < alive/2) {\n",
        "            // rank is a receiver\n",
        "            long long rank_sum;\n",
        "            MPI_Status status;\n",
        "            int src = rank + alive/2;\n",
        "            MPI_Recv (&rank_sum, 1, MPI_LONG_LONG, src, 0, MPI_COMM_WORLD, &status);\n",
        "            sum += rank_sum;\n",
        "        } else if (rank < alive) {\n",
        "            // rank is a sender\n",
        "            int dest = rank - alive/2;\n",
        "            MPI_Send (&sum, 1, MPI_LONG_LONG, dest, 0, MPI_COMM_WORLD);\n",
        "        }\n",
        "        alive = alive/2;\n",
        "    }\n",
        "\n",
        "    // all nonzero ranks receive the final sum from rank 0\n",
        "    if (rank == 0) {\n",
        "        for (int dest = 1;dest < size;dest++) {\n",
        "            MPI_Send(&sum,1,MPI_LONG_LONG,dest,0,MPI_COMM_WORLD);\n",
        "        }\n",
        "    } else {\n",
        "        int src = 0;\n",
        "        MPI_Status status;\n",
        "        MPI_Recv(&sum,1,MPI_LONG_LONG,src,0,MPI_COMM_WORLD,&status);\n",
        "    }\n",
        "\n",
        "    // stop the timer\n",
        "    end_time = MPI_Wtime();\n",
        "\n",
        "    // print results\n",
        "    printf (\"rank %d (of %d) sum = %lld, N*(N+1)/2 = %lld, elapsed time = %.4f seconds\\n\",\n",
        "            rank,size,sum,(N/2)*(N+1),end_time-start_time);\n",
        "\n",
        "    MPI_Finalize();\n",
        "}"
      ],
      "metadata": {
        "id": "I2oRT8Rbx_aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here are the results of running version 2 on matrix with N equal to 1 million.\n",
        "\n",
        "    $ mpicc -o mpi_sum_v2 mpi_sum_v2.c\n",
        "    $ mpiexec -n 4 ./mpi_sum_v2 1000000\n",
        "    rank 0 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds\n",
        "    rank 1 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds\n",
        "    rank 2 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds\n",
        "    rank 3 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds\n",
        "    $"
      ],
      "metadata": {
        "id": "RVvvnJLJ0jNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In version 3, we use parallel communication to broadcast the sum on rank 0 to all other ranks.\n",
        "## For simplicity, we assume that the number of ranks is $2^k$ where $k \\geq 0$."
      ],
      "metadata": {
        "id": "n1Ecmo1T4fX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_sum_v3.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "\n",
        "    MPI_Init (&argc, &argv);\n",
        "\n",
        "    // MPI_COMM_WORLD is the default communicator that contains all ranks\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // get N from command line\n",
        "    if (argc < 2) {\n",
        "        printf (\"Command usage : %s %s\\n\",argv[0],\"N\");\n",
        "        return 1;\n",
        "    }\n",
        "    long long N = atoll(argv[1]);\n",
        "\n",
        "    // start the timer\n",
        "    double start_time, end_time;\n",
        "    start_time = MPI_Wtime();\n",
        "\n",
        "    // calculate the sum\n",
        "    long long sum = 0;\n",
        "    for (long long i = 1+rank; i <= N;i+=size) {\n",
        "        sum += i;\n",
        "    }\n",
        "\n",
        "    // use parallel message passing to reduce the partial sums with result on rank 0\n",
        "    // we assume that size = 2^k for some integer k >= 0\n",
        "    int alive = size;\n",
        "    while (alive > 1) {\n",
        "        if (rank < alive/2) {\n",
        "            // rank is a receiver\n",
        "            long long rank_sum;\n",
        "            MPI_Status status;\n",
        "            int src = rank + alive/2;\n",
        "            MPI_Recv (&rank_sum, 1, MPI_LONG_LONG, src, 0, MPI_COMM_WORLD, &status);\n",
        "            sum += rank_sum;\n",
        "        } else if (rank < alive) {\n",
        "            // rank is a sender\n",
        "            int dest = rank - alive/2;\n",
        "            MPI_Send (&sum, 1, MPI_LONG_LONG, dest, 0, MPI_COMM_WORLD);\n",
        "        }\n",
        "        alive = alive/2;\n",
        "    }\n",
        "\n",
        "    // use parallel message passing to broadcast the sum on rank 0 to all other ranks\n",
        "    // we assume that size = 2^k for some integer k >= 0\n",
        "    alive = 1;\n",
        "    while (alive < size) {\n",
        "        alive = alive*2;\n",
        "        if (rank < alive/2) {\n",
        "            // rank is a sender\n",
        "            int dest = rank + alive/2;\n",
        "            MPI_Send (&sum, 1, MPI_LONG_LONG, dest, 0, MPI_COMM_WORLD);\n",
        "        } else if (rank < alive) {\n",
        "            // rank is a receiver */\n",
        "            MPI_Status status;\n",
        "            int src = rank - alive/2;\n",
        "            MPI_Recv (&sum, 1, MPI_LONG_LONG, src, 0, MPI_COMM_WORLD, &status);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // stop the timer\n",
        "    end_time = MPI_Wtime();\n",
        "\n",
        "    // print results\n",
        "    printf (\"rank %d (of %d) sum = %lld, N*(N+1)/2 = %lld, elapsed time = %.4f seconds\\n\",\n",
        "            rank,size,sum,(N/2)*(N+1),end_time-start_time);\n",
        "\n",
        "    MPI_Finalize();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiE76r6Q0kW7",
        "outputId": "82cffeac-50e1-44c0-fdd6-f5bbcc030297"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_sum_v3.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here are the results of running version 3 on matrix with N equal to 1 million.\n",
        "\n",
        "    $ mpicc -o mpi_sum_v3 mpi_sum_v3.c\n",
        "    $ mpiexec -n 4 ./mpi_sum_v3 1000000\n",
        "    rank 0 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0009 seconds\n",
        "    rank 1 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0009 seconds\n",
        "    rank 2 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0009 seconds\n",
        "    rank 3 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0009 seconds\n",
        "    $"
      ],
      "metadata": {
        "id": "C4QcUTvp42Ma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 : Comparing the Performance of Parallel and Sequential Message Passing"
      ],
      "metadata": {
        "id": "g0Jy5R0z5N8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To compare the performance of parallel message passing versus sequential message passing we need an MPI code that does a lot of communication!\n",
        "\n",
        "## In the sequential version of our MPI test code we will have a configurable number of communication rounds.  During each communication round, each rank will generate a random number and send that number to rank 0 so that it can be added to the total sum.  \n",
        "\n",
        "## Here is the code that implements the MPI communications using sequential message passing.  Note that only rank 0 will have the correct total sum.\n",
        "\n",
        "    int total_sum = 0;\n",
        "    for (int round = 0;round < rounds;round++) {\n",
        "\t    int round_sum = random() % 5;\n",
        "\t    if (rank == 0) {\n",
        "\t        int number;\n",
        "            MPI_Status status;\n",
        "\t        for (int src = 1;src < size;src++) {\n",
        "\t\t        MPI_Recv(&number,1,MPI_INT,src,0,MPI_COMM_WORLD,&status);\n",
        "\t\t        round_sum += number;\n",
        "\t        }\n",
        "\t    } else {\n",
        "\t        int dest = 0;\n",
        "\t        MPI_Send(&round_sum,1,MPI_INT,dest,0,MPI_COMM_WORLD);\n",
        "\t    }\n",
        "\t    total_sum += round_sum;\n",
        "    }\n"
      ],
      "metadata": {
        "id": "sQQewdZCw65h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To thoroughly test sequential message passing performance we run our MPI code on 64 ranks where each rank is on a different compute node!  \n",
        "\n",
        "## This ensures that point-to-point communications between ranks will need to use the network.\n",
        "\n",
        "## Tests involving such heavy use of ARC resources need to be run during non-peak hours.  \n",
        "\n",
        "## Here is the part of the *sbatch* file that asks for the computing resources used during our test.\n",
        "\n",
        "    #!/bin/bash\n",
        "    #SBATCH -A cmda3634_rjh\n",
        "    #SBATCH -p normal_q\n",
        "    #SBATCH -t 5\n",
        "    #SBATCH --nodes=64\n",
        "    #SBATCH --ntasks-per-node=1\n",
        "    #SBATCH -o mpi_seq.out"
      ],
      "metadata": {
        "id": "qPGEkqF7yTeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here is the sequential performance (500000 rounds, 64 ranks on 64 different compute nodes).\n",
        "\n",
        "    [jasonwil@tinkercliffs2 L24]$ sbatch mpi_seq.sh 500000 1234\n",
        "    Submitted batch job 2300140\n",
        "    [jasonwil@tinkercliffs2 L24]$ squeue -u jasonwil\n",
        "        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "        2300140  normal_q mpi_seq. jasonwil  R       0:04     64 tc[009-010,\n",
        "        015-016,024-027,035-037,049-053,066-067,070-071,079-087,089-090,099-101,\n",
        "        170-171,179-182,197-203,229-234,236-237,242-246,259-260,266-267]\n",
        "    [jasonwil@tinkercliffs2 L24]$ cat mpi_seq.out\n",
        "    elapsed time = 8.1717 seconds\n",
        "    rounds = 500000, seed = 1234, sum = 64020814\n"
      ],
      "metadata": {
        "id": "5B7gpzC--o9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To compare sequential message passing to parallel message passing we modify our MPI communications to perform the sum reduction in parallel:\n",
        "\n",
        "    int total_sum = 0;\n",
        "    for (int round = 0;round < rounds;round++) {\n",
        "\t    int round_sum = random() % 5;\n",
        "\t    // use parallel message passing to reduce the partial sums with result on rank 0\n",
        "\t    // we assume that size = 2^k for some integer k >= 0\n",
        "\t    int alive = size;\n",
        "\t    while (alive > 1) {\n",
        "\t        if (rank < alive/2) {\n",
        "\t\t        // rank is a receiver\n",
        "\t\t        int rank_sum;\n",
        "\t\t        MPI_Status status;\n",
        "\t\t        int src = rank + alive/2;\n",
        "\t\t        MPI_Recv (&rank_sum, 1, MPI_INT, src, 0, MPI_COMM_WORLD, &status);\n",
        "\t\t        round_sum += rank_sum;\n",
        "\t        } else if (rank < alive) {\n",
        "\t\t        // rank is a sender\n",
        "\t\t        int dest = rank - alive/2;\n",
        "\t\t        MPI_Send (&round_sum, 1, MPI_INT, dest, 0, MPI_COMM_WORLD);\n",
        "\t        }\n",
        "\t        alive = alive/2;\n",
        "\t    }\n",
        "\t    total_sum += round_sum;\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JUH4Hqyl1t0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here is the parallel performance (500000 rounds, 64 ranks on 64 different compute nodes).\n",
        "\n",
        "    [jasonwil@tinkercliffs2 L24]$ sbatch mpi_par.sh 500000 1234\n",
        "    Submitted batch job 2300076\n",
        "    [jasonwil@tinkercliffs2 L21]$ squeue -u jasonwil\n",
        "        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "        2300076  normal_q mpi_par. jasonwil  R       0:02     64 tc[009-010,\n",
        "        015-016,035-037,049-053,066-067,070-071,074-075,079-087,089-090,099-101,\n",
        "        113-114,170-171,179-182,197-201,229-234,236-237,242-246,259-260,266-267,\n",
        "        272-273]\n",
        "    [jasonwil@tinkercliffs2 L21]$ cat mpi_par.out\n",
        "    elapsed time = 0.8618 seconds\n",
        "    rounds = 500000, seed = 1234, sum = 64020814\n"
      ],
      "metadata": {
        "id": "F_mKL7xN_KFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note that the speedup when using parallel message passing versus sequential is:\n",
        "\n",
        "$$\\text{speedup} = 8.1717/0.8618 = 9.5$$\n",
        "\n",
        "## We could have (roughly) predicted this speedup as follows.  \n",
        "\n",
        "## Since $2^6 = 64$ we recall from lab 23 that the sequential code will perform 63 sequential communication steps per round while the parallel code will perform 6 parallel communication steps per round.\n",
        "\n",
        "## Note that the ideal speedup of $63/6 = 10.5$ is only slightly larger than the actual speedup of $9.5$.\n",
        "\n",
        "## Note that the performance advantage when using parallel message passing grows very rapidly as the number of ranks increases.  \n",
        "\n",
        "## For example if the number of ranks is 1024, then since $2^{10} = 1024$ the ideal speedup is $1023/10 = 102$.  "
      ],
      "metadata": {
        "id": "M9PtSykw2_aS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 : Introduction to Collective Communication"
      ],
      "metadata": {
        "id": "1DI4JcTe_x0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## As shown in part 2, parallel message passing is much more efficient than sequential message passing.\n",
        "\n",
        "## However, note that the code complexity of version 3 is quite high and we are assuming the special case where the number of ranks is 2^k for simplicity.  \n",
        "\n",
        "## Fortunately, the type of communications we are doing in version 3 (i.e. reducing partial sums and broadcasting) are so common in parallel computing that MPI has built-in **collective communication functions** that drastically simplify the communications code part of our MPI programs.  \n",
        "\n",
        "## In addition, these collective communication functions use parallel message passing under the hood so they are very efficient as well.  \n"
      ],
      "metadata": {
        "id": "ublE6sMopDZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The problem of reducing partial values on all ranks to a final value on a given rank using some type of operation (i.e. sum, min, max, etc.) is called a **reduction**.  \n",
        "\n",
        "## More specifically, the problem of adding partial sums on all ranks to compute the total sum on a given rank is called a **sum reduction**.\n",
        "\n",
        "## The MPI_Reduce function in MPI performs a reduction in one line of code.\n",
        "\n",
        "## For example, to add the partial sums stored in each rank in the variable *rank_sum* and put the result in the variable *sum* on rank 0 we use the code:\n",
        "\n",
        "    // use collective communication to reduce the partial sums with result on rank 0\n",
        "    long long rank_sum = sum;\n",
        "    MPI_Reduce(&rank_sum,&sum,1,MPI_LONG_LONG,MPI_SUM,0,MPI_COMM_WORLD);\n",
        "\n",
        "## For reference, here is the interface to MPI_Reduce\n",
        "\n",
        "    int MPI_Reduce(const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm);\n",
        "\n",
        "## The problem of copying the value of a variable on some rank to all other ranks is called a **broadcast**.\n",
        "\n",
        "## The MPI_Bcast function in MPI performs a broadcast in one line of code.\n",
        "\n",
        "## For example, to copy the total sum stored in rank 0 in the variable *sum* to the variable *sum* in all other ranks we use the code:\n",
        "\n",
        "    // use collective communication to broadcast sum on rank 0 to all ranks\n",
        "    MPI_Bcast(&sum,1,MPI_LONG_LONG,0,MPI_COMM_WORLD);\n",
        "\n",
        "## For reference, here is the interface to MPI_Bcast\n",
        "\n",
        "    int MPI_Bcast( void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm );\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3vhE2nqBqCAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here is version 4 of our sum code which replaces the parallel message passing code with calls to MPI_Reduce and MPI_Bcast.  Note how much easier the code is to read and understand!"
      ],
      "metadata": {
        "id": "tynxb1cMtuFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_sum_v4.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "\n",
        "    MPI_Init (&argc, &argv);\n",
        "\n",
        "    // MPI_COMM_WORLD is the default communicator that contains all ranks\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // get N from command line\n",
        "    if (argc < 2) {\n",
        "        printf (\"Command usage : %s %s\\n\",argv[0],\"N\");\n",
        "        return 1;\n",
        "    }\n",
        "    long long N = atoll(argv[1]);\n",
        "\n",
        "    // start the timer\n",
        "    double start_time, end_time;\n",
        "    start_time = MPI_Wtime();\n",
        "\n",
        "    // calculate the sum\n",
        "    long long sum = 0;\n",
        "    for (long long i = 1+rank; i <= N;i+=size) {\n",
        "        sum += i;\n",
        "    }\n",
        "\n",
        "    // use collective communication to reduce the partial sums with result on rank 0\n",
        "    long long rank_sum = sum;\n",
        "    MPI_Reduce(&rank_sum,&sum,1,MPI_LONG_LONG,MPI_SUM,0,MPI_COMM_WORLD);\n",
        "\n",
        "    // use collective communication to broadcast sum on rank 0 to all ranks\n",
        "    MPI_Bcast(&sum,1,MPI_LONG_LONG,0,MPI_COMM_WORLD);\n",
        "\n",
        "    // stop the timer\n",
        "    end_time = MPI_Wtime();\n",
        "\n",
        "    // print results\n",
        "    printf (\"rank %d (of %d) sum = %lld, N*(N+1)/2 = %lld, elapsed time = %.4f seconds\\n\",\n",
        "            rank,size,sum,(N/2)*(N+1),end_time-start_time);\n",
        "\n",
        "    MPI_Finalize();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50q-FKB34wG8",
        "outputId": "e9240f0c-4797-4022-8952-f596a0005ea1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_sum_v4.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here are the results of running version 4 on matrix with N equal to 1 million.\n",
        "\n",
        "    $ mpicc -o mpi_sum_v4 mpi_sum_v4.c\n",
        "    $ mpiexec -n 4 ./mpi_sum_v4 1000000\n",
        "    rank 0 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds\n",
        "    rank 1 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds\n",
        "    rank 2 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds\n",
        "    rank 3 (of 4) sum = 500000500000, N*(N+1)/2 = 500000500000, elapsed time = 0.0010 seconds"
      ],
      "metadata": {
        "id": "VxWqVx1cuMi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 : Performance of MPI_Reduce"
      ],
      "metadata": {
        "id": "2uUTsrgDvXIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To test the performance of MPI_Reduce compared to the parallel and sequential message passing codes from part 2 we use the collective communication code:\n",
        "\n",
        "    int total_sum = 0;\n",
        "    for (int round = 0;round < rounds;round++) {\n",
        "\t    int round_sum = random() % 5;\n",
        "\t    int number = round_sum;\n",
        "\t    MPI_Reduce(&number,&round_sum,1,MPI_INT,MPI_SUM,0,MPI_COMM_WORLD);\n",
        "\t    total_sum += round_sum;\n",
        "    }\n"
      ],
      "metadata": {
        "id": "RogWU9qw7WdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance using MPI_Reduce (500000 rounds, 64 ranks on 64 different compute nodes).\n",
        "\n",
        "    [jasonwil@tinkercliffs2 L24]$ sbatch mpi_reduce.sh 500000 1234\n",
        "    [jasonwil@tinkercliffs2 L24]$ squeue -u jasonwil\n",
        "        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "        2300544  normal_q mpi_redu jasonwil  R       0:02     64 tc[003-004,006,\n",
        "        009-010,012-013,015-016,018,020,022,024-027,032,035,037,046,049-053,056,\n",
        "        059,061,066-067,070-071,075,079-087,089-090,095,099-101,104,133-134,\n",
        "        179-181,196-201,239-240,253-254]\n",
        "    [jasonwil@tinkercliffs2 L21]$ cat mpi_reduce.out\n",
        "    elapsed time = 1.1595 seconds\n",
        "    rounds = 500000, seed = 1234, sum = 64020814\n"
      ],
      "metadata": {
        "id": "aW4qmS3EvmDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The runtime when using MPI_Reduce is comparable to that of our hand-coded parallel message passing version.  **This indicates that MPI_Reduce is using parallel message passing!**\n",
        "\n",
        "## Also note that MPI_Reduce easily handles an arbitrary number of ranks (recall that we assumed that the number of ranks was $2^k$ to simplify our parallel message passing code)."
      ],
      "metadata": {
        "id": "-9XpnnJK8B68"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4m9BhRIuTaP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}